##############################################
##############################################
##############################################
if np.all(copyValueMap == valueMap):

Policy evaluation finished in Iteration 1633
[[-18.16963976   0.         -29.21986398 -44.06359076 -51.55885323
  -54.68018408]
 [-32.33927952 -30.16764659 -39.59600116 -47.41205508 -51.93278484
  -53.80151493]
 [-44.6805522  -44.73530566 -47.58443901 -50.05584357 -50.95871613
  -50.79157587]
 [-52.96707143 -52.50858485 -51.95060565 -50.26816405 -47.05466022
  -43.61449656]
 [-57.71207723 -56.38135668 -53.44123467 -48.01154676 -39.37726417
  -28.99725357]
 [-59.7878036  -57.86352996 -53.42142959 -44.95952415 -29.44559611
    0.        ]]

Policy Iteration 1
Policy evaluation finished in Iteration 1633
[[-18.16963976   0.         -29.21986398 -44.06359076 -51.55885323
  -54.68018408]
 [-32.33927952 -30.16764659 -39.59600116 -47.41205508 -51.93278484
  -53.80151493]
 [-44.6805522  -44.73530566 -47.58443901 -50.05584357 -50.95871613
  -50.79157587]
 [-52.96707143 -52.50858485 -51.95060565 -50.26816405 -47.05466022
  -43.61449656]
 [-57.71207723 -56.38135668 -53.44123467 -48.01154676 -39.37726417
  -28.99725357]
 [-59.7878036  -57.86352996 -53.42142959 -44.95952415 -29.44559611
    0.        ]]

Policy Iteration 2
Policy evaluation finished in Iteration 7
[[-1.  0. -1. -2. -3. -4.]
 [-2. -1. -2. -3. -4. -4.]
 [-3. -2. -3. -4. -4. -3.]
 [-4. -3. -4. -4. -3. -2.]
 [-5. -4. -4. -3. -2. -1.]
 [-6. -4. -3. -2. -1.  0.]]

Policy Iteration 3
Policy evaluation finished in Iteration 2
[[-1.  0. -1. -2. -3. -4.]
 [-2. -1. -2. -3. -4. -4.]
 [-3. -2. -3. -4. -4. -3.]
 [-4. -3. -4. -4. -3. -2.]
 [-5. -4. -4. -3. -2. -1.]
 [-5. -4. -3. -2. -1.  0.]]


 The result is: 
 [[-1.  0. -1. -2. -3. -4.]
 [-2. -1. -2. -3. -4. -4.]
 [-3. -2. -3. -4. -4. -3.]
 [-4. -3. -4. -4. -3. -2.]
 [-5. -4. -4. -3. -2. -1.]
 [-5. -4. -3. -2. -1.  0.]] 
 [[0. 0. 1. 0.]
 [1. 0. 0. 0.]
 [0. 0. 0. 1.]
 [0. 0. 0. 1.]
 [0. 0. 0. 1.]
 [0. 0. 0. 1.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [0. 1. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [0. 1. 0. 0.]
 [0. 1. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [0. 1. 0. 0.]
 [0. 1. 0. 0.]
 [0. 1. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [0. 1. 0. 0.]
 [0. 1. 0. 0.]
 [0. 1. 0. 0.]
 [0. 1. 0. 0.]
 [0. 0. 1. 0.]
 [0. 0. 1. 0.]
 [0. 0. 1. 0.]
 [0. 0. 1. 0.]
 [0. 0. 1. 0.]
 [1. 0. 0. 0.]]
Simulation finished. Congratulations,sir!




##############################################
##############################################
##############################################
if np.all(np.abs(copyValueMap - valueMap) < 1e-6):

Policy evaluation finished in Iteration 676
[[-18.16962761   0.         -29.21984254 -44.06355769 -51.55881395
  -54.68014211]
 [-32.33925622 -30.16762448 -39.59597171 -47.41201931 -51.93274528
  -53.80147374]
 [-44.68051851 -44.73527182 -47.58440293 -50.05580553 -50.95867737
  -50.79153724]
 [-52.96703026 -52.50854415 -51.95056562 -50.26812566 -47.05462464
  -43.6144638 ]
 [-57.71203151 -56.38131227 -53.44119306 -48.01151002 -39.37723466
  -28.99723223]
 [-59.78775579 -57.86348402 -53.42138778 -44.95948971 -29.44557419
    0.        ]]

Policy Iteration 1
Policy evaluation finished in Iteration 676
[[-18.16962761   0.         -29.21984254 -44.06355769 -51.55881395
  -54.68014211]
 [-32.33925622 -30.16762448 -39.59597171 -47.41201931 -51.93274528
  -53.80147374]
 [-44.68051851 -44.73527182 -47.58440293 -50.05580553 -50.95867737
  -50.79153724]
 [-52.96703026 -52.50854415 -51.95056562 -50.26812566 -47.05462464
  -43.6144638 ]
 [-57.71203151 -56.38131227 -53.44119306 -48.01151002 -39.37723466
  -28.99723223]
 [-59.78775579 -57.86348402 -53.42138778 -44.95948971 -29.44557419
    0.        ]]

Policy Iteration 2
Policy evaluation finished in Iteration 7
[[-1.  0. -1. -2. -3. -4.]
 [-2. -1. -2. -3. -4. -4.]
 [-3. -2. -3. -4. -4. -3.]
 [-4. -3. -4. -4. -3. -2.]
 [-5. -4. -4. -3. -2. -1.]
 [-6. -4. -3. -2. -1.  0.]]

Policy Iteration 3
Policy evaluation finished in Iteration 2
[[-1.  0. -1. -2. -3. -4.]
 [-2. -1. -2. -3. -4. -4.]
 [-3. -2. -3. -4. -4. -3.]
 [-4. -3. -4. -4. -3. -2.]
 [-5. -4. -4. -3. -2. -1.]
 [-5. -4. -3. -2. -1.  0.]]


 The result is: 
 [[-1.  0. -1. -2. -3. -4.]
 [-2. -1. -2. -3. -4. -4.]
 [-3. -2. -3. -4. -4. -3.]
 [-4. -3. -4. -4. -3. -2.]
 [-5. -4. -4. -3. -2. -1.]
 [-5. -4. -3. -2. -1.  0.]] 
 [[0. 0. 1. 0.]
 [1. 0. 0. 0.]
 [0. 0. 0. 1.]
 [0. 0. 0. 1.]
 [0. 0. 0. 1.]
 [0. 0. 0. 1.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [0. 1. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [0. 1. 0. 0.]
 [0. 1. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [0. 1. 0. 0.]
 [0. 1. 0. 0.]
 [0. 1. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [0. 1. 0. 0.]
 [0. 1. 0. 0.]
 [0. 1. 0. 0.]
 [0. 1. 0. 0.]
 [0. 0. 1. 0.]
 [0. 0. 1. 0.]
 [0. 0. 1. 0.]
 [0. 0. 1. 0.]
 [0. 0. 1. 0.]
 [1. 0. 0. 0.]]
Simulation finished. Congratulations,sir!





##############################################
##############################################
##############################################

if np.all(np.abs(copyValueMap - valueMap) < 1e-2):

Policy evaluation finished in Iteration 234
[[-18.04695161   0.         -29.00338622 -43.72954938 -51.16208248
  -54.25629634]
 [-32.10403449 -29.94438335 -39.29848545 -47.05076375 -51.53316605
  -53.38551378]
 [-44.34019444 -44.39344995 -47.21997658 -49.67168864 -50.5673037
  -50.40143128]
 [-52.55120476 -52.097475   -51.54637864 -49.88043309 -46.69525068
  -43.28369352]
 [-57.25028602 -55.93281504 -53.02100994 -47.64043221 -39.07925153
  -28.78171549]
 [-59.30497181 -57.39952859 -52.99911495 -44.61167998 -29.22421686
    0.        ]]

Policy Iteration 1
Policy evaluation finished in Iteration 234
[[-18.04695161   0.         -29.00338622 -43.72954938 -51.16208248
  -54.25629634]
 [-32.10403449 -29.94438335 -39.29848545 -47.05076375 -51.53316605
  -53.38551378]
 [-44.34019444 -44.39344995 -47.21997658 -49.67168864 -50.5673037
  -50.40143128]
 [-52.55120476 -52.097475   -51.54637864 -49.88043309 -46.69525068
  -43.28369352]
 [-57.25028602 -55.93281504 -53.02100994 -47.64043221 -39.07925153
  -28.78171549]
 [-59.30497181 -57.39952859 -52.99911495 -44.61167998 -29.22421686
    0.        ]]

Policy Iteration 2
Policy evaluation finished in Iteration 7
[[-1.  0. -1. -2. -3. -4.]
 [-2. -1. -2. -3. -4. -4.]
 [-3. -2. -3. -4. -4. -3.]
 [-4. -3. -4. -4. -3. -2.]
 [-5. -4. -4. -3. -2. -1.]
 [-6. -4. -3. -2. -1.  0.]]

Policy Iteration 3
Policy evaluation finished in Iteration 2
[[-1.  0. -1. -2. -3. -4.]
 [-2. -1. -2. -3. -4. -4.]
 [-3. -2. -3. -4. -4. -3.]
 [-4. -3. -4. -4. -3. -2.]
 [-5. -4. -4. -3. -2. -1.]
 [-5. -4. -3. -2. -1.  0.]]


 The result is: 
 [[-1.  0. -1. -2. -3. -4.]
 [-2. -1. -2. -3. -4. -4.]
 [-3. -2. -3. -4. -4. -3.]
 [-4. -3. -4. -4. -3. -2.]
 [-5. -4. -4. -3. -2. -1.]
 [-5. -4. -3. -2. -1.  0.]] 
 [[0. 0. 1. 0.]
 [1. 0. 0. 0.]
 [0. 0. 0. 1.]
 [0. 0. 0. 1.]
 [0. 0. 0. 1.]
 [0. 0. 0. 1.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [0. 1. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [0. 1. 0. 0.]
 [0. 1. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [0. 1. 0. 0.]
 [0. 1. 0. 0.]
 [0. 1. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [0. 1. 0. 0.]
 [0. 1. 0. 0.]
 [0. 1. 0. 0.]
 [0. 1. 0. 0.]
 [0. 0. 1. 0.]
 [0. 0. 1. 0.]
 [0. 0. 1. 0.]
 [0. 0. 1. 0.]
 [0. 0. 1. 0.]
 [1. 0. 0. 0.]]
Simulation finished. Congratulations,sir!






##############################################
##############################################
##############################################

if np.all(np.abs(copyValueMap - valueMap) < 1e-1):

Policy evaluation finished in Iteration 124
[[-16.9524843    0.         -27.07213728 -40.7494543  -47.62232523
  -50.47459743]
 [-30.0054732  -27.95268023 -36.64432323 -43.82759795 -47.96802775
  -49.6741977 ]
 [-41.30396622 -41.34382789 -43.96862732 -46.24460188 -47.07542009
  -46.92082763]
 [-48.84141171 -48.43008743 -47.94034099 -46.42149657 -43.48891757
  -40.33252909]
 [-53.13084365 -51.93154232 -49.27230103 -44.32977592 -36.42068595
  -26.85887541]
 [-54.99785152 -53.26036498 -49.2317876  -41.50864477 -27.24932388
    0.        ]]

Policy Iteration 1
Policy evaluation finished in Iteration 124
[[-16.9524843    0.         -27.07213728 -40.7494543  -47.62232523
  -50.47459743]
 [-30.0054732  -27.95268023 -36.64432323 -43.82759795 -47.96802775
  -49.6741977 ]
 [-41.30396622 -41.34382789 -43.96862732 -46.24460188 -47.07542009
  -46.92082763]
 [-48.84141171 -48.43008743 -47.94034099 -46.42149657 -43.48891757
  -40.33252909]
 [-53.13084365 -51.93154232 -49.27230103 -44.32977592 -36.42068595
  -26.85887541]
 [-54.99785152 -53.26036498 -49.2317876  -41.50864477 -27.24932388
    0.        ]]

Policy Iteration 2
Policy evaluation finished in Iteration 7
[[-1.  0. -1. -2. -3. -4.]
 [-2. -1. -2. -3. -4. -4.]
 [-3. -2. -3. -4. -4. -3.]
 [-4. -3. -4. -4. -3. -2.]
 [-5. -4. -4. -3. -2. -1.]
 [-6. -4. -3. -2. -1.  0.]]

Policy Iteration 3
Policy evaluation finished in Iteration 2
[[-1.  0. -1. -2. -3. -4.]
 [-2. -1. -2. -3. -4. -4.]
 [-3. -2. -3. -4. -4. -3.]
 [-4. -3. -4. -4. -3. -2.]
 [-5. -4. -4. -3. -2. -1.]
 [-5. -4. -3. -2. -1.  0.]]


 The result is: 
 [[-1.  0. -1. -2. -3. -4.]
 [-2. -1. -2. -3. -4. -4.]
 [-3. -2. -3. -4. -4. -3.]
 [-4. -3. -4. -4. -3. -2.]
 [-5. -4. -4. -3. -2. -1.]
 [-5. -4. -3. -2. -1.  0.]] 
 [[0. 0. 1. 0.]
 [1. 0. 0. 0.]
 [0. 0. 0. 1.]
 [0. 0. 0. 1.]
 [0. 0. 0. 1.]
 [0. 0. 0. 1.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [0. 1. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [0. 1. 0. 0.]
 [0. 1. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [0. 1. 0. 0.]
 [0. 1. 0. 0.]
 [0. 1. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [0. 1. 0. 0.]
 [0. 1. 0. 0.]
 [0. 1. 0. 0.]
 [0. 1. 0. 0.]
 [0. 0. 1. 0.]
 [0. 0. 1. 0.]
 [0. 0. 1. 0.]
 [0. 0. 1. 0.]
 [0. 0. 1. 0.]
 [1. 0. 0. 0.]]
Simulation finished. Congratulations,sir!

Process finished with exit code 0

